Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	analyze_signalp
	1	extract_signalp
	1	prep_and_run_signalp
	3

rule prep_and_run_signalp:
    input: bos_taurus.txt
    output: bos_taurus_sigp.out, ensembl_parsed_bos_taurus
    jobid: 1
    wildcards: organism=bos_taurus

Finished job 1.
1 of 3 steps (33%) done

rule analyze_signalp:
    input: bos_taurus_sigp.out
    output: bos_taurus_signalp_positives.out
    jobid: 2
    wildcards: organism=bos_taurus

Finished job 2.
2 of 3 steps (67%) done

rule extract_signalp:
    input: ensembl_parsed_bos_taurus, bos_taurus_signalp_positives.out
    output: extracted_sigp_bos_taurus.out
    jobid: 0
    wildcards: organism=bos_taurus

Finished job 0.
3 of 3 steps (100%) done
Shutting down, this might take some time.
Complete log: /home/mstolarczyk/Uczelnia/MGR/master_thesis/Pipeline/.snakemake/log/2018-02-28T152046.001230.snakemake.log
